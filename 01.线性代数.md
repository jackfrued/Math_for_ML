## 线性代数

### 向量

**向量**（*vector*）也叫**矢量**，是一个同时具有大小和方向，且满足平行四边形法则的几何对象。与向量相对的概念叫**标量**或**数量**，标量只有大小，绝大多数情况下没有方向。我们通常用带箭头的线段来表示向量，在平面直角坐标系中的向量如下图所示。需要注意的是，向量是表达大小和方向的量，并没有规定起点和终点，所以相同的向量可以画在任意位置，例如下图中$\boldsymbol{w}$和$\boldsymbol{u}$两个向量并没有什么区别。

<img src="res/vector_1.png" style="zoom:40%;">

向量有很多种代数表示法，对于二维空间的向量，下面几种写法都是可以的。
$$
\boldsymbol{a} = \langle a_1, a_2 \rangle = (a_1, a_2) = \begin{pmatrix} a_1 \\ a_2 \end{pmatrix} = \begin{bmatrix} a_1 \\ a_2 \end{bmatrix}
$$
向量的大小称为向量的模，它是一个标量，对于二维空间的向量，模可以通过下面的公式计算。
$$
\lvert \boldsymbol{a} \rvert = \sqrt{a_{1}^{2} + a_{2}^{2}}
$$
注意，这里的$\lvert \boldsymbol{a} \rvert$并不是绝对值，你可以将其称为向量$\boldsymbol{a}$的二范数，这是数学中的符号重用现象。上面的写法和概念也可以推广到$n$维空间，我们通常用$\boldsymbol{R^n}$表示$n$维空间，我们刚才说的二维空间可以记为$\boldsymbol{R^2}$，三维空间可以记为$\boldsymbol{R^3}$。虽然生活在三维空间的我们很难想象四维空间、五维空间是什么样子，但是这并不影响我们探讨高维空间，机器学习中，我们经常把有$n$个特征的训练样本称为一个$n$维向量。

#### 向量的加法

相同维度的向量可以相加得到一个新的向量，运算的方法是将向量的每个分量相加，如下所示。
$$
\boldsymbol{u} = \begin{bmatrix} u_1 \\ u_2 \\ \vdots \\ u_n \end{bmatrix}, \quad
\boldsymbol{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}, \quad
\boldsymbol{u} + \boldsymbol{v} = \begin{bmatrix} u_1 + v_1 \\ u_2 + v_2 \\ \vdots \\ u_n + v_n \end{bmatrix}
$$
向量的加法满足“平行四边形法则”，即两个向量$\boldsymbol{u}$和$\boldsymbol{v}$构成了平行四边形的两条邻边，相加的结果是平行四边形的对角线，如下图所示。

<img src="res/vector_2.png" style="zoom:58%;">

#### 向量的数乘

一个向量$\boldsymbol{v}$可以和一个标量$k$相乘，运算的方法是将向量中的每个分量与该标量相乘即可，如下所示。
$$
\boldsymbol{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}, \quad
k \cdot \boldsymbol{v} = \begin{bmatrix} k \cdot v_1 \\ k \cdot v_2 \\ \vdots \\ k \cdot v_n \end{bmatrix}
$$
我们可以用 NumPy 的数组来表示向量，向量的加法可以通过两个数组的加法来实现，向量的数乘可以通过数组和标量的乘法来实现，此处不再进行赘述。

#### 向量的点积

点积（*dot product*）是两个向量之间最为重要的运算之一，运算的方法是将两个向量对应分量的乘积求和，所以点积的结果是一个标量，其几何意义是两个向量的模乘以二者夹角的余弦如下所示。
$$
\boldsymbol{u} = \begin{bmatrix} u_1 \\ u_2 \\ \vdots \\ u_n \end{bmatrix}, \quad
\boldsymbol{v} = \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix} \quad \\
\boldsymbol{u} \cdot \boldsymbol{v} = \sum_{i=1}^{n}{u_iv_i} = \lvert \boldsymbol{u} \rvert \lvert \boldsymbol{v} \rvert cos\theta
$$
假如我们用3维向量来表示用户对喜剧片、言情片和动作片这三类电影的偏好，我们用1到5的数字来表示喜欢的程度，其中5表示非常喜欢，4表示比较喜欢，3表示无感，2表示比较反感，1表示特别反感。那么，下面的向量表示用户非常喜欢喜剧片，特别反感言情片，对动作片不喜欢也不反感。
$$
\boldsymbol{u} = \begin{pmatrix} 5 \\ 1 \\ 3 \end{pmatrix}
$$
现在有两部电影上映了，一部属于言情喜剧片，一部属于喜剧动作片，我们把两部电影也通过3维向量的方式进行表示，如下所示。
$$
\boldsymbol{m_1} = \begin{pmatrix} 4 \\ 5 \\ 1 \end{pmatrix}, \quad \boldsymbol{m_2} = \begin{pmatrix} 5 \\ 1 \\ 5 \end{pmatrix}
$$
如果现在我们需要向刚才的用户推荐一部电影，我们应该给他推荐哪一部呢？我们可以将代表用户的向量$\boldsymbol{u}$和代表电影的向量$\boldsymbol{m_1}$和$\boldsymbol{m_2}$分别进行点积运算，再除以向量的模长，得到向量夹角的余弦值，余弦值越接近1，说明向量的夹角越接近0度，也就是两个向量的相似度越高。很显然，我们应该向用户推荐跟他观影喜好相似度更高的电影。
$$
cos\theta_1 = \frac{\boldsymbol{u} \cdot \boldsymbol{m1}}{|\boldsymbol{u}||\boldsymbol{m1}|} \approx \frac{4 \times 5 + 5 \times 1 + 3 \times 1}{5.92 \times 6.48} \approx 0.73 \\
cos\theta_2 = \frac{\boldsymbol{u} \cdot \boldsymbol{m2}}{|\boldsymbol{u}||\boldsymbol{m2}|} \approx \frac{5 \times 5 + 1 \times 1 + 3 \times 5}{5.92 \times 7.14} \approx 0.97
$$
大家可能会说，向量$\boldsymbol{m_2}$代表的电影肉眼可见跟用户是更加匹配的。的确，对于一个三维向量我们凭借直觉也能够给出正确的答案，但是对于一个$n$维向量，当$n$的值非常大时，你还有信心凭借肉眼的观察和本能的直觉给出准确的答案吗？向量的点积可以通过`dot`函数来计算，而向量的模长可以通过 NumPy 的`linalg`模块中的`norm`函数来计算，代码如下所示。

```python
u = np.array([5, 1, 3])
m1 = np.array([4, 5, 1])
m2 = np.array([5, 1, 5])
print(np.dot(u, m1) / (np.linalg.norm(u) * np.linalg.norm(m1)))  # 0.7302967433402214
print(np.dot(u, m2) / (np.linalg.norm(u) * np.linalg.norm(m2)))  # 0.9704311900788593
```

#### 向量的叉积

在二维空间，两个向量的叉积是这样定义的：
$$
\boldsymbol{A} = \begin{pmatrix} a_{1} \\ a_{2} \end{pmatrix}, \quad \boldsymbol{B} = \begin{pmatrix} b_{1} \\ b_{2} \end{pmatrix} \\
\boldsymbol{A} \times \boldsymbol{B} = \begin{vmatrix} a_{1} \quad a_{2} \\ b_{1} \quad b_{2} \end{vmatrix} = a_{1}b_{2} - a_{2}b_{1}
$$
对于三维空间，两个向量的叉积结果是一个向量，如下所示：
$$
\boldsymbol{A} = \begin{pmatrix} a_{1} \\ a_{2} \\ a_{3} \end{pmatrix}, \quad \boldsymbol{B} = \begin{pmatrix} b_{1} \\ b_{2} \\ b_{3} \end{pmatrix} \\
\boldsymbol{A} \times \boldsymbol{B} = \begin{vmatrix} \boldsymbol{\hat{i}} \quad \boldsymbol{\hat{j}} \quad \boldsymbol{\hat{k}} \\ a_{1} \quad a_{2} \quad a_{3} \\ b_{1} \quad b_{2} \quad b_{3} \end{vmatrix} = \langle \boldsymbol{\hat{i}}\begin{vmatrix} a_{2} \quad a_{3} \\ b_{2} \quad b_{3} \end{vmatrix}, -\boldsymbol{\hat{j}}\begin{vmatrix} a_{1} \quad a_{3} \\ b_{1} \quad b_{3} \end{vmatrix}, \boldsymbol{\hat{k}}\begin{vmatrix} a_{1} \quad a_{2} \\ b_{1} \quad b_{2} \end{vmatrix} \rangle
$$
因为叉积的结果是向量，所以$\boldsymbol{A} \times \boldsymbol{B}$和$\boldsymbol{B} \times \boldsymbol{A}$的结果并不相同，事实上：
$$
\boldsymbol{A} \times \boldsymbol{B} = -(\boldsymbol{B} \times \boldsymbol{A})
$$
NumPy 中可以通过`cross`函数来计算向量的叉积，代码如下所示。

```python
print(np.cross(u, m1))  # [-14   7  21]
print(np.cross(m1, u))  # [ 14  -7 -21]
```

### 行列式

**行列式**（*determinant*）通常记作$det(\boldsymbol{A})$或$|\boldsymbol{A}|$，其中$\boldsymbol{A}$是一个$n$阶方阵。行列式可以看做是有向面积或体积的概念在一般欧几里得空间的推广，或者说行列式描述的是一个线性变换对“体积”所造成的影响。行列式的概念最早出现在解线性方程组的过程中，十七世纪晚期，关孝和（日本江户时代的数学家）与莱布尼茨的著作中已经使用行列式来确定线性方程组解的个数以及形式；十八世纪开始，行列式开始作为独立的数学概念被研究；十九世纪以后，行列式理论进一步得到发展和完善。

<img src="res/vector_3.jpg" style="zoom:125%;">

#### 行列式的性质

行列式是由向量引出的，所以行列式解释的其实是向量的性质。

**性质1**：如果$det(\boldsymbol{A})$中某行（或某列）的元素全部为0，那么$det(\boldsymbol{A}) = 0$。

**性质2**：如果$det(\boldsymbol{A})$中某行（或某列）有公共因子$k$，则可以提出$k$，得到行列式$det(\boldsymbol{A^{'}})$，且$det(\boldsymbol{A}) = k \cdot det(\boldsymbol{A^{'}})$。
$$
det(\boldsymbol{A})={\begin{vmatrix}a_{11}&a_{12}&\dots &a_{1n}\\\vdots &\vdots &\ddots &\vdots \\{\color {blue}k}a_{i1}&{\color {blue}k}a_{i2}&\dots &{\color {blue}k}a_{in}\\\vdots &\vdots &\ddots &\vdots \\a_{n1}&a_{n2}&\dots &a_{nn}\end{vmatrix}}={\color {blue}k}{\begin{vmatrix}a_{11}&a_{12}&\dots &a_{1n}\\\vdots &\vdots &\ddots &\vdots \\a_{i1}&a_{i2}&\dots &a_{in}\\\vdots &\vdots &\ddots &\vdots \\a_{n1}&a_{n2}&\dots &a_{nn}\end{vmatrix}}={\color {blue}k} \cdot det(\boldsymbol{A^{'}})
$$

**性质3**：如果$det(\boldsymbol{A})$中某行（或某列）的每个元素是两数之和，则此行列式可拆分为两个行列式相加，如下所示。
$$
{\begin{vmatrix}a_{11}&a_{12}&\dots &a_{1n}\\\vdots &\vdots &\ddots &\vdots \\{\color {blue}a_{i1}}+{\color {OliveGreen}b_{i1}}&{\color {blue}a_{i2}}+{\color {OliveGreen}b_{i2}}&\dots &{\color {blue}a_{in}}+{\color {OliveGreen}b_{in}}\\\vdots &\vdots &\ddots &\vdots \\a_{n1}&a_{n2}&\dots &a_{nn}\end{vmatrix}}={\begin{vmatrix}a_{11}&a_{12}&\dots &a_{1n}\\\vdots &\vdots &\ddots &\vdots \\{\color {blue}a_{i1}}&{\color {blue}a_{i2}}&\dots &{\color {blue}a_{in}}\\\vdots &\vdots &\ddots &\vdots \\a_{n1}&a_{n2}&\dots &a_{nn}\end{vmatrix}}+{\begin{vmatrix}a_{11}&a_{12}&\dots &a_{1n}\\\vdots &\vdots &\ddots &\vdots \\{\color {OliveGreen}b_{i1}}&{\color {OliveGreen}b_{i2}}&\dots &{\color {OliveGreen}b_{in}}\\\vdots &\vdots &\ddots &\vdots \\a_{n1}&a_{n2}&\dots &a_{nn}\end{vmatrix}}
$$
**性质4**：如果$det(\boldsymbol{A})$中两行（或两列）元素对应成比例，那么$det(\boldsymbol{A}) = 0$。

**性质5**：如果$det(\boldsymbol{A})$中两行（或两列）互换得到$det(\boldsymbol{A^{'}})$，那么$det(\boldsymbol{A}) = -det(\boldsymbol{A^{'}})$。

**性质6**：将$det(\boldsymbol{A})$中某行（或某列）的$k$倍加进另一行（或另一列）里，行列式的值不变，如下所示。
$$
{\begin{vmatrix}\vdots &\vdots &\vdots &\vdots \\a_{i1}&a_{i2}&\dots &a_{in}\\a_{j1}&a_{j2}&\dots &a_{jn}\\\vdots &\vdots &\vdots &\vdots \\\end{vmatrix}}={\begin{vmatrix}\vdots &\vdots &\vdots &\vdots \\a_{i1}&a_{i2}&\dots &a_{in}\\a_{j1}{\color {blue}+ka_{i1}}&a_{j2}{\color {blue}+ka_{i2}}&\dots &a_{jn}{\color {blue}+ka_{in}}\\\vdots &\vdots &\vdots &\vdots \\\end{vmatrix}}
$$
**性质7**：将行列式的行列互换，行列式的值不变，如下所示。
$$
{\begin{vmatrix}a_{11}&a_{12}&\dots &a_{1n}\\a_{21}&a_{22}&\dots &a_{2n}\\\vdots &\vdots &\ddots &\vdots \\a_{n1}&a_{n2}&\dots &a_{nn}\end{vmatrix}}={\begin{vmatrix}a_{11}&a_{21}&\dots &a_{n1}\\a_{12}&a_{22}&\dots &a_{n2}\\\vdots &\vdots &\ddots &\vdots \\a_{1n}&a_{2n}&\dots &a_{nn}\end{vmatrix}}
$$
**性质8**：方块矩阵$\boldsymbol{A}$和$\boldsymbol{B}$的乘积的行列式等于其行列式的乘积，即$det(\boldsymbol{A}\boldsymbol{B}) = det(\boldsymbol{A})det(\boldsymbol{B})$。特别的，若将矩阵中的每一行都乘以常数$r$，那么行列式的值将是原来的$r^{n}$倍，即$det(r\boldsymbol{A}) = det(r\boldsymbol{I_{n}} \cdot \boldsymbol{A}) = r^{n}det(\boldsymbol{A})$，其中$\boldsymbol{I_{n}}$是$n$阶单位矩阵。

**性质9**：若$\boldsymbol{A}$是可逆矩阵，那么$det(\boldsymbol{A}^{-1}) = (det(\boldsymbol{A}))^{-1}$。

#### 行列式的计算

$n$阶行列式的计算公式如下所示：
$$
det(\boldsymbol{A})=\sum_{n!} \pm {a_{1\alpha}a_{2\beta} \cdots a_{n\omega}}
$$

对于二阶行列式，上面的公式相当于：
$$
\begin{vmatrix} a_{11} \quad a_{12} \\ a_{21} \quad a_{22} \end{vmatrix} = a_{11}a_{22} - a_{12}a_{21}
$$
对于三阶行列式，上面的计算公式相当于：
$$
\begin{vmatrix} a_{11} \quad a_{12} \quad a_{13} \\ a_{21} \quad a_{22} \quad a_{23} \\ a_{31} \quad a_{32} \quad a_{33} \end{vmatrix} = a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} - a_{11}a_{23}a_{32} - a_{12}a_{21}a_{33} - a_{13}a_{22}a_{31}
$$
高阶行列式可以用**代数余子式**（*cofactor*）展开成多个低阶行列式，如下所示：
$$
det(\boldsymbol{A})=a_{11}C_{11}+a_{12}C_{12}+ \cdots +a_{1n}C_{1n} = \sum_{i=1}^{n}{a_{1i}C_{1i}}
$$
其中，$C_{11}$是原行列式去掉$a_{11}$所在行和列之后剩余的部分构成的行列式，以此类推。

### 矩阵

**矩阵**（*matrix*）是由一系列元素排成的矩形阵列，矩阵里的元素可以是数字、符号或数学公式。矩阵可以进行**加法**、**减法**、**数乘**、**转置**、**矩阵乘法**等运算，如下图所示。

<img src="res/matrix_operation.png" style="zoom:62%;">

值得一提的是矩阵乘法运算，该运算仅当第一个矩阵$\boldsymbol{A}$的列数和另一个矩阵$\boldsymbol{B}$的行数相等时才能定义。如果$\boldsymbol{A}$是一个$m \times n$的矩阵，$\boldsymbol{B}$是一个$n \times k$矩阵，它们的乘积是一个$m \times k$的矩阵，其中元素的计算公式如下所示：
$$
 [\mathbf{AB}]_{i,j} = A_{i,1}B_{1,j} + A_{i,2}B_{2,j} + \cdots + A_{i,n}B_{n,j} = \sum_{r=1}^n A_{i,r}B_{r,j}
$$
<img src="res/matrix_multiply.png" style="zoom:35%;">

例如：
$$
\begin{bmatrix}
    1 & 0 & 2 \\
    -1 & 3 & 1 \\
  \end{bmatrix}
\times
  \begin{bmatrix}
    3 & 1 \\
    2 & 1 \\
    1 & 0
  \end{bmatrix}
=
  \begin{bmatrix}
     (1 \times 3  +  0 \times 2  +  2 \times 1) & (1 \times 1   +   0 \times 1   +   2 \times 0) \\
    (-1 \times 3  +  3 \times 2  +  1 \times 1) & (-1 \times 1   +   3 \times 1   +   1 \times 0) \\
  \end{bmatrix}
=
  \begin{bmatrix}
    5 & 1 \\
    4 & 2 \\
  \end{bmatrix}
$$
矩阵的乘法满足结合律和对矩阵加法的分配律：

结合律： $(\boldsymbol{AB})\boldsymbol{C} = \boldsymbol{A}(\boldsymbol{BC})$。

左分配律：$(\boldsymbol{A} + \boldsymbol{B})\boldsymbol{C} = \boldsymbol{AC} + \boldsymbol{BC}$。

右分配律：$\boldsymbol{C}(\boldsymbol{A} + \boldsymbol{B}) = \boldsymbol{CA} + \boldsymbol{CB}$。

**矩阵乘法不满足交换律**。一般情况下，矩阵$\boldsymbol{A}$和$\boldsymbol{B}$的乘积$\boldsymbol{AB}$存在，但$\boldsymbol{BA}$不一定存在，即便$\boldsymbol{BA}$存在，大多数时候$\boldsymbol{AB} \neq \boldsymbol{BA}$。

矩阵乘法的一个基本应用是在线性方程组上。线性方程组是方程组的一种，它符合以下的形式：
$$
\begin{cases}
     a_{1,1}x_{1} + a_{1,2}x_{2} + \cdots + a_{1,n}x_{n}=  b_{1} \\
     a_{2,1}x_{1} + a_{2,2}x_{2} + \cdots + a_{2,n}x_{n}=  b_{2} \\
     \vdots \quad \quad \quad \vdots \\
     a_{m,1}x_{1} + a_{m,2}x_{2} + \cdots + a_{m,n}x_{n}=  b_{m}
 \end{cases}
$$
运用矩阵的方式，可以将线性方程组写成一个向量方程：
$$
\boldsymbol{Ax} = \boldsymbol{b}
$$
其中，$\boldsymbol{A}$是由方程组里未知数的系数排成的$m \times n$矩阵，$\boldsymbol{x}$是含有$n$个元素的行向量，$\boldsymbol{b}$是含有$m$个元素的行向量。

矩阵是线性变换（保持向量加法和标量乘法的函数）的便利表达法。矩阵乘法的本质在联系到线性变换的时候最能体现，因为矩阵乘法和线性变换的合成有以下的联系，即每个$m \times n$的矩阵$\boldsymbol{A}$都代表了一个从$\boldsymbol{R}^{n}$射到$\boldsymbol{R}^{m}$的线性变换。如果无法理解上面这些内容，推荐大家看看B站上名为[《线性代数的本质》](https://www.bilibili.com/video/BV1ib411t7YR/)的视频，相信这套视频会让你对线性代数有一个更好的认知。

下图是一个来自于维基百科的例子，图中展示了一些典型的二维实平面上的线性变换对平面向量（图形）造成的效果以及它们对应的二维矩阵，其中每个线性变换将蓝色图形映射成绿色图形；平面的原点$(0, 0)$用黑点表示。

<img src="res/linear_transformation.png" style="zoom:45%;">
