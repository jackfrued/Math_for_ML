## 信息论

### 信息熵

1948年，克劳德·香农（Claude Shannon）在他的著名论文《通信的数学原理》中提出了“信息熵”的概念，它解决了信息的度量问题，量化出了信息的价值。“熵”（entropy）原本是热力学领域的概念，它反映的是系统的混乱程度，熵越大，系统的混乱程度就越高。在信息论中，熵可以看作随机变量不确定性的度量，变量的不确定性越大，熵就越大，那么要确定它所需要的信息量也就越大。

例如，甲、乙两人参加一个射击比赛，如果从历史成绩来看，甲的胜率是100%，那么我们很容易接受甲会获胜这个结果；如果从历史成绩来看，甲的胜率是50%，那么我们就难以确定谁会获胜。克劳德·香农提出可以用下面的公式来描述这种不确定性：
$$
H = -\sum_{x \in X} P(x)log_2P(x)
$$
我们用$ x_1 $和$ x_2 $来分别表示甲和乙获胜，很显然，当$  P(x_1)=0.5 $，$ P(x_2)=0.5 $时，$ H=1 $；当$ P(x_1)=1 $，$ P(x_2)=0 $时，$ H=0 $，当$ P(x_1) = 0.8 $，$ P(x2) = 0.2 $时，$ H \approx 0.72 $。

如果要表示两个随机变量的熵，可以定义联合熵，如下所示：
$$
H(X,Y) = -\sum_{x \in X}\sum_{y \in Y} P(x, y)log_2P(x,y)
$$
很显然，知道的信息越多，随机事件的不确定性就越小。这些信息，可以是直接针对我们想了解的随机事件的信息，也可以是和我们关心的随机事件相关的其他事件的信息。在数学上可以严格的证明这些相关的信息也能够消除不确定性。为此，要先引入条件熵的概念，如下所示：
$$
H(X|Y) = -\sum_{x \in X}\sum_{y \in Y}P(x, y)log_2P(x|y)
$$
可以证明$ H(x) \ge H(X|Y)$，也就是说多了$ Y $的信息之后，关于$ X $的不确定性下降了。当然，还要注意等号成立的情况，也就是说增加了$ Y $的信息，但是$ X $的不确定没有下降，也就是说我们获取的信息与要研究的内容没有关系。

根据上面的公式，我们还可以做出以下的推导：
$$
H(X, Y) = -\sum_{x \in X}\sum_{y \in Y} P(x, y)log_2P(x,y) \\
= -\sum_{x \in X}\sum_{y \in Y} P(x, y)log_2P(y)P(x|y) \\
= -\sum_{x \in X}\sum_{y \in Y} P(x, y)log_2P(y) -\sum_{x \in X}\sum_{y \in Y} P(x, y)log_2P(x|y) \\
= -\sum_{x \in X} P(x)log_2P(y) -\sum_{x \in X}\sum_{y \in Y} P(x, y)log_2P(x|y) \\
= H(X) + H(X|Y)
$$
从上面的推导可以看出，对于两个随机变量的随机系统，我们可以先观察一个随机变量获取信息量，观察完后，我们可以在拥有这个信息量的基础上观察第二个随机变量的信息量，而且先观察谁，对信息量都不会有影响。上面的式子也告诉了我们：
$$
H(X|Y) = H(X, Y) - H(X)
$$
刚才我们说到除了直接的信息之外，相关的信息也会帮助我们消除不确定性，那么我们如何量化这种相关性呢？香农在信息论中提出了一个“互信息”（Mutual Information）的概念作为两个随机变量相关性的量化，例如在夏天的时候，我们经常说“好闷热啊，要下雨了”，这里的“闷热”和“下雨”的互信息就很高。对于随机变量X和Y，它们的互信息定义如下所示：
$$
I(X;Y) = \sum_{x \in X}\sum_{y \in Y}P(x, y)\log_2\frac{P(x, y)}{P(x)P(y)}
$$
可以证明，互信息就是随机变量$ X $的不确定性$ H(X) $跟知道了随机变量$ Y $的条件下$ X $的不确定性$ H(X|Y) $之间的差异，即：
$$
I(X;Y) = \sum_{x \in X}\sum_{y \in Y}P(x, y)\log_2\frac{P(x, y)}{P(x)P(y)} \\
= \sum_{x \in X}\sum_{y \in Y}P(x, y)\log_2\frac{P(x|y)P(y)}{P(x)P(y)} \\
= \sum_{x \in X}\sum_{y \in Y}P(x, y)\log_2\frac{P(x|y)}{P(x)} \\
= \sum_{x \in X}\sum_{y \in Y}P(x, y)\log_2P(x|y) - \sum_{x \in X}\sum_{y \in Y}P(x, y)\log_2P(x) \\
= -\sum_{x \in X}P(x)\log_2P(x) - (-\sum_{x \in X}\sum_{y \in Y}P(x, y)\log_2P(x|y)) \\
= H(X) - H(X|Y)
$$
很显然，互信息是一个取值在0到$ H(X) $之间的函数，当$ X $和$ Y $完全相关时，它的取值是$ H(X) $；当$ X $和$ Y $完全无关时，它的取值是0。
